#  Financial Data Lake & Economic Indicators ETL

Este proyecto implementa una arquitectura de datos h铆brida dise帽ada para simular operaciones cr铆ticas de un Banco Retail. Combina el procesamiento masivo de transacciones locales (Big Data) con la ingesta de indicadores econ贸micos en la nube (Cloud Data Engineering).

##  Descripci贸n del Negocio

El sistema resuelve dos necesidades fundamentales de la banca moderna:
1.  **An谩lisis Transaccional (Core Bancario):** Procesamiento de grandes vol煤menes de transacciones para detectar fraude, calcular riesgo crediticio y medir eficiencia operativa.
2.  **Tesorer铆a y Riesgo de Mercado:** Actualizaci贸n diaria de indicadores econ贸micos cr铆ticos (UF, D贸lar) necesarios para la valoraci贸n de pasivos y cr茅ditos hipotecarios.

---

##  Arquitectura del Proyecto

El repositorio opera como un **Monorepo** con dos pipelines desacoplados:

### M贸dulo 1: Bank Data Lake (PySpark)
Motor de procesamiento distribuido contenedorizado.
* **Ingesta (Bronze):** Generaci贸n de datos sint茅ticos con inyecci贸n controlada de errores (ruido/calidad).
* **Limpieza (Silver):** Pipeline de Calidad de Datos que elimina duplicados y nulos, particionando la data por fecha (`Hive Style Partitioning`).
* **Explotaci贸n (Gold):** C谩lculo de KPIs de negocio:
    *  **Alerta de Fraude:** Detecci贸n de movimientos inusuales (AML).
    *  **Clientes VIP:** Scoring para fidelizaci贸n y aumento de cupo.
    *  **Eficiencia de Sucursales:** M茅tricas operativas.

### M贸dulo 2: Indicadores Econ贸micos (Python + AWS)
ETL de misi贸n cr铆tica para datos de mercado.
* **Extracci贸n:** Conexi贸n a la API de `mindicador.cl` (Fuente oficial Banco Central de Chile).
* **Indicadores:** UF, D贸lar Observado, Euro, UTM.
* **Carga:** Persistencia hist贸rica en **AWS RDS (PostgreSQL)** para consumo de sistemas financieros.

---

##  Tech Stack

* **Lenguaje:** Python 3.9
* **Big Data:** Apache Spark (PySpark)
* **Infraestructura:** Docker & Docker Compose
* **Nube:** AWS RDS (PostgreSQL), AWS S3
* **Calidad de Datos:** Scripts de auditor铆a automatizada
* **Versionamiento:** Git / GitHub

---

##  C贸mo Ejecutar el Proyecto

### Prerrequisitos
* Docker Desktop instalado y corriendo.
* Archivo `.env` configurado con credenciales de AWS.

### 1. Ejecutar Pipeline Bancario (Data Lake)
Este comando genera datos, los limpia, audita la calidad y calcula los KPIs.
```bash
docker run --rm -v ${PWD}/output:/app/output financial-etl sh -c "python src/bank_pyspark/generar_datos.py && python src/bank_pyspark/etl_limpieza.py && python src/bank_pyspark/calculo_kpis.py"
